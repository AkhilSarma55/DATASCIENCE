{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3alsTF1GQvT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** 1.What is the Naive Approach in machine learning?**\n",
        "\n"
      ],
      "metadata": {
        "id": "WO1X9PGfGRlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A naive classifier model is one that does not use any sophistication in order to make a prediction, typically making a random or constant prediction. Such models are naive because they don't use any knowledge about the domain or any learning in order to make a prediction."
      ],
      "metadata": {
        "id": "6FyxMsWcGY2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2.Explain the assumptions of feature independence in the Naive Approach.*"
      ],
      "metadata": {
        "id": "2xJXhNFJGm2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature"
      ],
      "metadata": {
        "id": "nHlAjdvgGs2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3. How does the Naive Approach handle missing values in the data?*"
      ],
      "metadata": {
        "id": "L0pBAPa9G25z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naïve Bayes Imputation (NBI) is used to fill in missing values by replacing the attribute information according to the probability estimate. The NBI process divides the whole data into two sub-sets is the complete data and data containing missing data. Complete data is used for the imputation process at the lost value."
      ],
      "metadata": {
        "id": "wl55YpsPG7S4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What are the advantages and disadvantages of the Naive Approach?"
      ],
      "metadata": {
        "id": "dnr2XhyxHEBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is simple and easy to implement\n",
        "\n",
        "It doesn't require as much training data.\n",
        "\n",
        "It handles both continuous and discrete data.\n",
        "\n",
        "It is highly scalable with the number of predictors and data points.\n",
        "\n",
        "It is fast and can be used to make real-time predictions."
      ],
      "metadata": {
        "id": "ypknSGNNHJ0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 5.Can the Naive Approach be used for regression problems? If yes, how?*"
      ],
      "metadata": {
        "id": "3HnX_D_EHb7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a supervised classification algorithm that is used primarily for dealing with binary and multi-class classification problems, though with some modifications, it can also be used for solving regression problems."
      ],
      "metadata": {
        "id": "UPG8ttygHi35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6. How do you handle categorical features in the Naive Approach?*"
      ],
      "metadata": {
        "id": "2EuuiPqFHt0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Drop columns with categorical data. You'll get started with the most straightforward approach. ...\n",
        "\n",
        "Step 2: Label encoding. Before jumping into label encoding, we'll investigate the dataset. ...\n",
        "\n",
        "Step 3: Investigating cardinality. ...\n",
        "\n",
        "Step 4: One-hot encoding."
      ],
      "metadata": {
        "id": "oLlGVRSKHy0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*7. What is Laplace smoothing and why is it used in the Naive Approach?*"
      ],
      "metadata": {
        "id": "VAr8I-_UIFWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. Using higher alpha values will push the likelihood towards a value of 0.5, i.e., the probability of a word equal to 0.5 for both the positive and negative reviews"
      ],
      "metadata": {
        "id": "aOJj0nDfIK8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*8. How do you choose the appropriate probability threshold in the Naive Approach?*"
      ],
      "metadata": {
        "id": "xFEwQNpcIQ2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The default value for the minimum threshold of probabilities is 0.001. Valid values are positive numbers that are smaller than 1. The Naive Bayes method can only work with discrete input fields."
      ],
      "metadata": {
        "id": "sNp4SEOcIhuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*9. Give an example scenario where the Naive Approach can be applied.*"
      ],
      "metadata": {
        "id": "k7hle1KCIite"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is used for Credit Scoring.\n",
        "\n",
        "It is used in medical data classification.\n",
        "\n",
        "It can be used in real-time predictions because Naïve Bayes Classifier is an eager learner.\n",
        "\n",
        "It is used in Text classification such as Spam filtering and Sentiment analysis."
      ],
      "metadata": {
        "id": "C1JECgkXIokx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*10. What is the K-Nearest Neighbors (KNN) algorithm?*"
      ],
      "metadata": {
        "id": "W6LwF35eIwSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
      ],
      "metadata": {
        "id": "US-0QfRUJDsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. How does the KNN algorithm work?\n"
      ],
      "metadata": {
        "id": "4S9fAMEJJJX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, the distance between the new point and each training point is calculated.\n",
        "\n",
        "The closest k data points are selected (based on the distance). ...\n",
        "\n",
        "The average of these data points is the final prediction for the new point."
      ],
      "metadata": {
        "id": "5kgw0sh2JQO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*12. How do you choose the value of K in KNN?*"
      ],
      "metadata": {
        "id": "94m3OgYaJYeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value of k is very crucial in the KNN algorithm to define the number of neighbors in the algorithm. The value of k in the k-nearest neighbors (k-NN) algorithm should be chosen based on the input data. If the input data has more outliers or noise, a higher value of k would be better."
      ],
      "metadata": {
        "id": "U0PRFHs8JhIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the advantages and disadvantages of the KNN algorithm?\n"
      ],
      "metadata": {
        "id": "-WFYCrE4Jlxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages\t                                     Disadvantages\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Simple and Easy to Understand\t                  Sensitive to Outliers\n",
        "Non-parametric\t                                Computationally Expensive\n",
        "No Training Required\t                          Requires Good Choice of K\n",
        "Can Handle Large Datasets\t                     Limited to Euclidean Distance"
      ],
      "metadata": {
        "id": "eQHVTx29JsDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*14. How does the choice of distance metric affect the performance of KNN?*"
      ],
      "metadata": {
        "id": "KVSGvCcAKMbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing with other distances. Representation of HasD between the point 0 and n, where n belongs to [À10,10]\n",
        "\n",
        "The k-NN classifier works by computing the distance between the unlabeled datum point and also the majority of comparable data points among k-nearest neighbors that are closed thereto query point. The distance between the test sample and the training data samples is calculated by a specific distance metric."
      ],
      "metadata": {
        "id": "dTwXmP86KVDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*15. Can KNN handle imbalanced datasets? If yes, how?*"
      ],
      "metadata": {
        "id": "uxaUFf9bKhNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In k Nearest Neighbor (kNN) classifier, a query instance is classified based on the most frequent class of its nearest neighbors among the training instances. In imbalanced datasets, kNN becomes biased towards the majority instances of the training space."
      ],
      "metadata": {
        "id": "wdC4gYrRKrIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*16. How do you handle categorical features in KNN?*"
      ],
      "metadata": {
        "id": "E-g1h1RNKwW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to decide how to convert categorical features to a numeric scale, and somehow assign inter-category distances in a way that makes sense with other features (like, age-age distances...but what is an age-category distance?)"
      ],
      "metadata": {
        "id": "llxusQdIKzt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*17. What are some techniques for improving the efficiency of KNN?*"
      ],
      "metadata": {
        "id": "idy4D5rILDuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model.\n",
        "\n",
        "The improved method is to sample the training data around the test data, which reduces the number of distance calculation of the test data to each training data, and reduces the time complexity of the algorithm.\n",
        "\n",
        "kNN algorithm computes the distance between each training sample and test samples in the dataset and then returns k closest samples. Its time complexity is linearly and is guaranteed to find exact k nearest neighbors"
      ],
      "metadata": {
        "id": "aATZ7AcRLKfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*18. Give an example scenario where KNN can be applied.*"
      ],
      "metadata": {
        "id": "C4LoiU1BQCHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the help of KNN algorithms, we can classify a potential voter into various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party 'Congress', “Will Vote to Party 'BJP'. Other areas in which KNN algorithm can be used are Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition.\n",
        "\n",
        "Many companies make a personalized recommendation for its consumers, such as Netflix, Amazon, YouTube, and many more. KNN can search for semantically similar documents. Each document is considered as a vector"
      ],
      "metadata": {
        "id": "SFS0QL41PXo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*19. What is clustering in machine learning?*"
      ],
      "metadata": {
        "id": "GF44tYTPPm7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other group.\n",
        "\n",
        "Clustering is the task of dividing the unlabeled data or data points into different clusters such that similar data points fall in the same cluster than those which differ from the others. In simple words, the aim of the clustering process is to segregate groups with similar traits and assign them into clusters."
      ],
      "metadata": {
        "id": "DCDiM_ZZPsNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*20. Explain the difference between hierarchical clustering and k-means clustering.*"
      ],
      "metadata": {
        "id": "FAndybOtP6g_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of 'K'. Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster.\n",
        "\n",
        "The two main types of classification are K-Means clustering and Hierarchical Clustering. K-Means is used when the number of classes is fixed, while the latter is used for an unknown number of classes. Distance is used to separate observations into different groups in clustering algorithms."
      ],
      "metadata": {
        "id": "PmCgt8gwQtJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*21. How do you determine the optimal number of clusters in k-means clustering?*"
      ],
      "metadata": {
        "id": "MrTaAN1EQ_iR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute clustering algorithm (e.g., k-means clustering) for different values of k. ...\n",
        "\n",
        "For each k, calculate the total within-cluster sum of square (wss).\n",
        "\n",
        "Plot the curve of wss according to the number of clusters k.\n",
        "\n"
      ],
      "metadata": {
        "id": "UuyTaVcNRS5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*22. What are some common distance metrics used in clustering?*"
      ],
      "metadata": {
        "id": "dmztM5aBRfkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance metrics are used in supervised and unsupervised learning to calculate similarity in data points. They improve the performance, whether that's for classification tasks or clustering. The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance.\n",
        "\n",
        "K-means Clustering does not work well with outliers and noisy datasets. DBScan clustering efficiently handles outliers and noisy datasets. 5. In the domain of anomaly detection, this algorithm causes problems as anomalous points will be assigned to the same cluster as “normal” data points"
      ],
      "metadata": {
        "id": "43dBx7LERxdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*23. How do you handle categorical features in clustering?*"
      ],
      "metadata": {
        "id": "Kw7DrB3KR7qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Pick K observations at random and use them as leaders/clusters.\n",
        "\n",
        "Step 2: Calculate the dissimilarities(no. of mismatches) and assign each observation to its closest cluster.\n",
        "\n",
        "Step 3: Define new modes for the clusters.\n",
        "\n",
        "Creating Toy Dataset.\n",
        "\n",
        "Step 1: Drop columns with categorical data. You'll get started with the most straightforward approach. ...\n",
        "\n",
        "Step 2: Label encoding. Before jumping into label encoding, we'll investigate the dataset. ...\n",
        "\n",
        "Step 3: Investigating cardinality. ...\n",
        "\n",
        "Step 4: One-hot encoding."
      ],
      "metadata": {
        "id": "oV7OUxwNSHkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advantage of Hierarchical Clustering is we don't have to pre-specify the clusters. However, it doesn't work very well on vast amounts of data or huge datasets. And there are some disadvantages of the Hierarchical Clustering algorithm that it is not suitable for large datasets\n",
        "\n",
        "The weaknesses are that it rarely provides the best solution, it involves lots of arbitrary decisions, it does not work with missing data, it works poorly with mixed data types, it does not work well on very large data sets, and its main output, the dendrogram, is commonly misinterpreted."
      ],
      "metadata": {
        "id": "R3R0CT30Sk4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*25. Explain the concept of silhouette score and its interpretation in clustering.*"
      ],
      "metadata": {
        "id": "pQvYmFc9TGj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
        "\n",
        "The silhouette score falls within the range [-1, 1]. The silhouette score of 1 means that the clusters are very dense and nicely separated. The score of 0 means that clusters are overlapping. The score less than 0 means that data belonging to clusters may be wrong / incorrect"
      ],
      "metadata": {
        "id": "PGmGsmIuTK5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*26. Give an example scenario where clustering can be applied.*"
      ],
      "metadata": {
        "id": "HAgFbaPvTX8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying Fake News. Fake news is not a new phenomenon, but it is one that is becoming prolific. ...\n",
        "\n",
        "Spam filter. ...\n",
        "\n",
        "Marketing and Sales. ...\n",
        "\n",
        "Classifying network traffic. ...\n",
        "\n",
        "Identifying fraudulent or criminal activity. ...\n",
        "\n",
        "Document analysis. ...\n",
        "\n",
        "Fantasy Football and Sports"
      ],
      "metadata": {
        "id": "9Cd6WOjYTd8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*27. What is anomaly detection in machine learning?*"
      ],
      "metadata": {
        "id": "x22za834Tj6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection is the identification of rare events, items, or observations which are suspicious because they differ significantly from standard behaviors or patterns. Anomalies in data are also called standard deviations, outliers, noise, novelties, and exceptions.\n",
        "\n",
        "A step in data mining that identifies data points, events, and/or observations that deviate from a dataset's normal behavior."
      ],
      "metadata": {
        "id": "ZOzXWTvUTrF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*28. Explain the difference between supervised and unsupervised anomaly detection.*"
      ],
      "metadata": {
        "id": "e6gPmCw0T2zK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The main difference between supervised and unsupervised anomaly detection is the approach involved, where supervised approach makes use of predefined algorithms and AI training, while unsupervised approach uses a general outlier-detection mechanism based on pattern matching."
      ],
      "metadata": {
        "id": "WqgkUg2zT97r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*29. What are some common techniques used for anomaly detection?*"
      ],
      "metadata": {
        "id": "FYBhLVzVUMcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical (Z-score, Tukey's range test and Grubbs's test)\n",
        "\n",
        "Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\n",
        "\n",
        "Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data."
      ],
      "metadata": {
        "id": "whtRZSeUUt9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*30. How does the One-Class SVM algorithm work for anomaly detection?*"
      ],
      "metadata": {
        "id": "TrvHgEiVU7wZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Popular non-parametric anomaly detection approaches include one-class SVM and density-based algorithms. One-class SVM is computationally efficient, but has no direct control of false alarm rate and usually gives unsatisfactory results.\n",
        "\n",
        "One-class SVM, or unsupervised SVM, is an algorithm used for anomaly detection. The algorithm tries to separate data from the origin in the transformed high-dimensional predictor space. ocsvm finds the decision boundary based on the primal form of SVM with the Gaussian kernel approximation method."
      ],
      "metadata": {
        "id": "oq01SCvkVCjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*31. How do you choose the appropriate threshold for anomaly detection?*"
      ],
      "metadata": {
        "id": "yESXXFbWVrNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Anomaly detection threshold, choose the number to use for the anomaly detection threshold. A higher number creates a thicker band of \"normal\" values that is more tolerant of metric changes. A lower number creates a thinner band that will go to ALARM state with smaller metric deviations.\n",
        "\n",
        " Simple statistical techniques such as mean, median, quantiles can be used to detect univariate anomalies feature values in the dataset. Various data visualization and exploratory data analysis techniques can be also be used to detect anomalies."
      ],
      "metadata": {
        "id": "7aRUB6B2YSp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*32. How do you handle imbalanced datasets in anomaly detection?*"
      ],
      "metadata": {
        "id": "9pvp5JzzYhFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose Proper Evaluation Metric. The accuracy of a classifier is the total number of correct predictions by the classifier divided by the total number of predictions. ...\n",
        "\n",
        "Resampling (Oversampling and Undersampling) ...\n",
        "\n",
        "SMOTE. ...\n",
        "\n",
        "BalancedBaggingClassifier. ...\n",
        "\n",
        "Threshold moving."
      ],
      "metadata": {
        "id": "XBK2yZubYogM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*33. Give an example scenario where anomaly detection can be applied.*"
      ],
      "metadata": {
        "id": "XlfZNNCjYzjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning.\n",
        "\n",
        "Image Processing.\n",
        "\n",
        "Fraud and Intrusion Detection.\n",
        "\n",
        "Threshold-Based Detection in Sensor Networks.\n",
        "\n",
        "Health Monitoring.\n",
        "\n",
        "Financial Transactions and Banking Applications.\n",
        "\n",
        "Ecosystem Turbulences.\n",
        "\n",
        "IT and Cyber Security.\n",
        "\n",
        "\n",
        "For example, a credit card company will use anomaly detection to track how customers typically use their credit cards. If a customer makes an abnormally large purchase or a purchase in a new location, the algorithm recognizes the anomaly and alerts a team member to contact the customer."
      ],
      "metadata": {
        "id": "eoyHxWMEZYZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*34. What is dimension reduction in machine learning?*"
      ],
      "metadata": {
        "id": "zWK_BRlkZwQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible. This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data."
      ],
      "metadata": {
        "id": "E8MkvoJCZ8DW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*35. Explain the difference between feature selection and feature extraction.*"
      ],
      "metadata": {
        "id": "1Lb1CJWCaGyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms, the feature extraction algorithms transform the data onto a new feature space"
      ],
      "metadata": {
        "id": "kV2URIUia6Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*36. How does Principal Component Analysis (PCA) work for dimension reduction?*"
      ],
      "metadata": {
        "id": "jNDO0SY9bDP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one."
      ],
      "metadata": {
        "id": "5Dfif-mNbIsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*37. How do you choose the number of components in PCA?*"
      ],
      "metadata": {
        "id": "9tsoRcZJbLMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset"
      ],
      "metadata": {
        "id": "CIniSzdrbZOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*38. What are some other dimension reduction techniques besides PCA?*"
      ],
      "metadata": {
        "id": "lRBpTNv0biCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection. ...\n",
        "\n",
        "Feature extraction. ...\n",
        "\n",
        "Principal Component Analysis (PCA) ...\n",
        "\n",
        "Non-negative matrix factorization (NMF) ...\n",
        "\n",
        "Linear discriminant analysis (LDA) ...\n",
        "\n",
        "Generalized discriminant analysis (GDA) ...\n",
        "\n",
        "Missing Values Ratio. ...\n",
        "\n",
        "Low Variance Filter"
      ],
      "metadata": {
        "id": "J2fg5xQ7buhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*39. Give an example scenario where dimension reduction can be applied.*"
      ],
      "metadata": {
        "id": "mrhJdVq_b8pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Principal Component Analysis (PCA), Factor Analysis (FA), Linear Discriminant Analysis (LDA) and Truncated Singular Value Decomposition (SVD) are examples of linear dimensionality reduction methods.\n",
        "\n",
        "\n",
        "Dimensionality reduction is advantageous to AI developers or data professionals working with massive datasets, performing data visualization and analyzing complex data. It aids in the process of data compression, allowing the data to take up less storage space as well as reduces computation times"
      ],
      "metadata": {
        "id": "FON9IooGcI_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*40. What is feature selection in machine learning?*"
      ],
      "metadata": {
        "id": "xPoMR3YmcWTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve."
      ],
      "metadata": {
        "id": "vPYrNQMKcheQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*41. Explain the difference between filter, wrapper, and embedded methods of feature selection.*"
      ],
      "metadata": {
        "id": "KInGWxIUcqcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Filter methods perform the feature selection independently of construction of the classification model. Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. In embedded methods the feature selection is an integral part of the classification model."
      ],
      "metadata": {
        "id": "AXQfVvp6c6kZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*42. How does correlation-based feature selection work?*"
      ],
      "metadata": {
        "id": "ruXz_Gtgjiew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features."
      ],
      "metadata": {
        "id": "i3gB2p8XjmH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*43. How do you handle multicollinearity in feature selection?*"
      ],
      "metadata": {
        "id": "AZt1xgtFjxHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing variables. A straightforward method of correcting multicollinearity is removing one or more variables showing a high correlation. ...\n",
        "More data. ...\n",
        "\n",
        "Using techniques such as partial least squares regression (PLS) and principal component analysis (PCA). ...\n",
        "\n",
        "Centering the variables."
      ],
      "metadata": {
        "id": "JveEiwTdj3IW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*44. What are some common feature selection metrics?*"
      ],
      "metadata": {
        "id": "2-5gd136kBV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three types of feature selection: Wrapper methods (forward, backward, and stepwise selection), Filter methods (ANOVA, Pearson correlation, variance thresholding), and Embedded methods (Lasso, Ridge, Decision Tree)."
      ],
      "metadata": {
        "id": "TH5tTY1okHlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*45. Give an example scenario where feature selection can be applied.*"
      ],
      "metadata": {
        "id": "CWcFSQh9bZZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example- Correlation-based Feature Selection, chi2 test, SelectKBest, and ANOVA F-value. – Embedded Methods: Select features by learning their importance during model training. For example- Lasso Regression, Ridge Regression, and Random Forest."
      ],
      "metadata": {
        "id": "QayOaULRZjyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*46. What is data drift in machine learning?*"
      ],
      "metadata": {
        "id": "7XGkVNfTLOLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Data drift is unexpected and undocumented changes to data structure, semantics, and infrastructure that is a result of modern data architectures. Data drift breaks processes and corrupts data, but can also reveal new opportunities for data use."
      ],
      "metadata": {
        "id": "kek0RgXvk8aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*47. Why is data drift detection important?*"
      ],
      "metadata": {
        "id": "fPB2Z3molUaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Over time, the model will make inductive generalizations based on a distribution that is not exactly what it was trained on, and performance will degrade. So, we need monitoring and (timely) detection of any data-drift in the MLOps or ModelOps pipeline for any (continually) successful ML model deployment"
      ],
      "metadata": {
        "id": "IlJ3rOcBlYaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*48. Explain the difference between concept drift and feature drift.*"
      ],
      "metadata": {
        "id": "dIN99OCWlec-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data drift refers to the changing distribution of the data to which the model is applied. Concept drift refers to a changing underlying goal or objective for the model. Both data drift and concept drift can lead to a decline in the performance of a machine learning model"
      ],
      "metadata": {
        "id": "gjOjJrutll-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*49. What are some techniques used for detecting data drift?*"
      ],
      "metadata": {
        "id": "tEWVnXZblphQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time distribution-based methods use statistical methods to calculate the difference between two probability distributions to detect drift. These methods include the Population Stability Index, KL Divergence, JS Divergence, KS Test, and the Wasserstein Metric."
      ],
      "metadata": {
        "id": "Cbyvib6llyXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*50. How can you handle data drift in a machine learning model?*"
      ],
      "metadata": {
        "id": "4tLZRQlhl3pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the data quality. Make sure the drift is real.\n",
        "Investigate. Explore the data changes to understand what might be causing them.\n",
        "Do nothing. ...\n",
        "Retrain it, if you can. ...\n",
        "Rebuild it, if you need. ...\n",
        "Use a fallback strategy. ...\n",
        "Limit the model use. ...\n",
        "Add custom processing logic\n",
        "\n",
        "Some strategies for addressing drift include continuously monitoring and evaluating the performance of a model, updating the model with new data, and using machine learning models that are more robust to drift."
      ],
      "metadata": {
        "id": "kIzAJB-zl-vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*51. What is data leakage in machine learning?*"
      ],
      "metadata": {
        "id": "szDAC0VJmJ4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple words, data leakage can be defined as: \"A scenario when ML model already has information of test data in training data, but this information would not be available at the time of prediction, called data leakage. It causes high performance while training set, but perform poorly in deployment or production.\""
      ],
      "metadata": {
        "id": "TRjUT50UmNi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*52. Why is data leakage a concern?*"
      ],
      "metadata": {
        "id": "DAh7sUS5mf2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“When data set contains relevant data, but similar data is not obtainable when the models are used for predictions, data leakage (or leaking) occurs. This results in great success on the training dataset (and possibly even the validation accuracy), but lack of performance in production."
      ],
      "metadata": {
        "id": "5LTPFUH0mo1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*53. Explain the difference between target leakage and train-test contamination.*"
      ],
      "metadata": {
        "id": "-zX-GaFQm1ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production."
      ],
      "metadata": {
        "id": "WVahh1QinAF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*54. How can you identify and prevent data leakage in a machine learning pipeline?*"
      ],
      "metadata": {
        "id": "XYKygrABnHZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prevent such leakage, the dataset should be separated into training and testing sets before performing any preprocessing steps. The preprocessing steps should only be fit on the training dataset and then applied to both the training and test datasets separately."
      ],
      "metadata": {
        "id": "thtUxPsGnfto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*55. What are some common sources of data leakage?*"
      ],
      "metadata": {
        "id": "G8RnbvY6nvk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Performing some kind of pre-processing on the full dataset whose results influence what is seen during training is one of the most common causes of data leakage"
      ],
      "metadata": {
        "id": "0sVnM9V6n1qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "56.*Give an example scenario where data leakage can occur.*"
      ],
      "metadata": {
        "id": "lE6oiW2yoaK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leakage occurs when information about the target label or number is introduced during learning that would not be lawfully accessible during actual use. The most fundamental example of data leakage would be if the true label of a dataset was included as a characteristic in the mode"
      ],
      "metadata": {
        "id": "484Hz52WpM0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "57. What is cross-validation in machine learning?"
      ],
      "metadata": {
        "id": "Q5HCzHlepc8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds."
      ],
      "metadata": {
        "id": "mGJ2lmkmpl-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "58. Why is cross-validation important?\n",
        "\n",
        "The main advantage of cross-validation is that it provides an estimate of the performance of the model on new data, which is important for assessing the model's generalizability. It also helps to avoid overfitting, which is a common problem in machine learning"
      ],
      "metadata": {
        "id": "zcAgcvsipp78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.*"
      ],
      "metadata": {
        "id": "AMORkpkdp1dO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Stratified k-fold cross-validation is the same as just k-fold cross-validation, But Stratified k-fold cross-validation, it does stratified sampling instead of random sampling"
      ],
      "metadata": {
        "id": "fcXgubaMp7u_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*60. How do you interpret the cross-validation results?*"
      ],
      "metadata": {
        "id": "-zO4I9p9qGyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the group as a holdout or test data set.\n",
        "\n",
        "Take the remaining groups as a training data set.\n",
        "\n",
        "Fit a model on the training set and evaluate it on the test set.\n",
        "\n",
        "Retain the evaluation score and discard the model."
      ],
      "metadata": {
        "id": "BA7dTvk7qM_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IjKR85rfqQt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yMlX4WeqpJsr"
      }
    }
  ]
}