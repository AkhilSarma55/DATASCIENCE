{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWIszVX-p2VL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1. How do word embeddings capture semantic meaning in text preprocessing?*"
      ],
      "metadata": {
        "id": "WCBUl_Lep4XF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embeddings in NLP is a technique where individual words are represented as real-valued vectors in a lower-dimensional space and captures inter-word semantics. Each word is represented by a real-valued vector with tens or hundreds of dimensions.\n",
        "\n",
        "Word Embeddings in NLP is a technique where individual words are represented as real-valued vectors in a lower-dimensional space and captures inter-word semantics. Each word is represented by a real-valued vector with tens or hundreds of dimensions."
      ],
      "metadata": {
        "id": "N1Lp4D5Ip_Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2. Explain the concept of recurrent neural networks (RNNs) and their role in text processing tasks.*"
      ],
      "metadata": {
        "id": "KDq1PwBsp_Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A recurrent neural network is a type of artificial neural network commonly used in speech recognition and natural language processing. Recurrent neural networks recognize data's sequential characteristics and use patterns to predict the next likely scenario.\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a form of machine learning algorithm that are ideal for sequential data such as text, time series, financial data, speech, audio, video among others. RNNs are ideal for solving problems where the sequence is more important than the individual items themselves."
      ],
      "metadata": {
        "id": "nGBsCu8kqOad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3. What is the encoder-decoder concept, and how is it applied in tasks like machine translation or text summarization?*"
      ],
      "metadata": {
        "id": "2Gl_2QWQqbix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder-decoder model is a way of using recurrent neural networks for sequence-to-sequence prediction problems. It was initially developed for machine translation problems, although it has proven successful at related sequence-to-sequence prediction problems such as text summarization and question answering.\n",
        "\n",
        "\n",
        "Encoders and Decoders for Neural Machine Translation ...\n",
        "The encoder is at the feeding end; it understands the sequence and reduces the dimension of the input sequence. The sequence has a fixed size known as the context vector. This context vector acts like input to the decoder, which generates an output sequence when reaching the end token."
      ],
      "metadata": {
        "id": "Vi24lOxQqjVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Discuss the advantages of attention-based mechanisms in text processing models.\n"
      ],
      "metadata": {
        "id": "dmlSCht5qp6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention mechanism allows the model to \"pay attention\" to certain parts of the data and to give them more weight when making predictions. In a nutshell, the attention mechanism helps preserve the context of every word in a sentence by assigning an attention weight relative to all other words."
      ],
      "metadata": {
        "id": "PRcg1XdqrACW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5. Explain the concept of self-attention mechanism and its advantages in natural language processing.*"
      ],
      "metadata": {
        "id": "8fpibmQrrQAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Self-Attention or K=V=Q, if the input is, for example, a sentence, then each word in the sentence needs to undergo Attention computation. The goal is to learn the dependencies between the words in the sentence and use that information to capture the internal structure of the sentence."
      ],
      "metadata": {
        "id": "3xR-4nDzrUoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6. What is the transformer architecture, and how does it improve upon traditional RNN-based models in text processing?*"
      ],
      "metadata": {
        "id": "TFCYMueprcEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Why are LSTMs struggling to matchup with Transformers? | by ...\n",
        "As discussed, transformers are faster than RNN-based models as all the input is ingested once. Training LSTMs is harder when compared with transformer networks, since the number of parameters is a lot more in LSTM networks. Moreover, it's impossible to do transfer learning in LSTM networks.\n",
        "\n",
        "An important advantage of the transformer architecture over other neural networks, such as RNNs is that the presence of context around a word over a longer distance is done in a more efficient way. Furthermore, unlike RNNs the input data needn't be processed sequentially, enabling for parallelisation"
      ],
      "metadata": {
        "id": "fmW3LXGPrf7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Describe the process of text generation using generative-based approaches."
      ],
      "metadata": {
        "id": "26NVgVLUrqA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A text-to-text Generative AI is an AI that Generates text based on text input. An example of a text-to-text Generative AI is ChatGPT, developed by OpenAI. Text generation uses machine learning, existing data and previous user input in generating responses.\n",
        "\n",
        "\n",
        "The generative model is a single platform for diversified areas of NLP that can address specific problems relating to read text, hear speech, interpret it, measure sentiment and determine which parts are important."
      ],
      "metadata": {
        "id": "CKrFQa7AruX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*8. What are some applications of generative-based approaches in text processing?*"
      ],
      "metadata": {
        "id": "gVpZrIrxsDJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text generation is a subfield of natural language processing (NLP) that deals with generating text automatically. It has a wide range of applications, including machine translation, content creation, and conversational agents. One of the most common text generation techniques is statistical language models."
      ],
      "metadata": {
        "id": "Jq-mG3DisO9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*9. Discuss the challenges and techniques involved in building conversation AI systems.*"
      ],
      "metadata": {
        "id": "7F_bARueshff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regional jargon and slang.\n",
        "\n",
        "Dialects not conforming to standard language.\n",
        "\n",
        "Background noise distorting the voice of the speaker.\n",
        "\n",
        "Unscripted questions that the virtual assistant or chatbot does not know to answer.\n",
        "\n",
        "Unplanned responses by customers."
      ],
      "metadata": {
        "id": "Pop4O8oqsjFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*10. How do you handle dialogue context and maintain coherence in conversation AI models?*"
      ],
      "metadata": {
        "id": "RKsKs4P_sp7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever the bot send a message, set(push) <a matcher method m(), a context callback> on the stack.\n",
        "\n",
        "On receiving a message, match the message using the matcher methods in FIFO order. Call the matched context callback."
      ],
      "metadata": {
        "id": "Q-KZh11TswVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*11. Explain the concept of intent recognition in the context of conversation AI.*"
      ],
      "metadata": {
        "id": "3Dm3h2yms4ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of intent classification is to analyze and then group the messages into “intents” that represent the information the user is looking for. Intent classification is a key aspect of the chatbot development process and it makes the bot capable of offering responses in a natural way.\n",
        "\n",
        "Intent recognition — sometimes called intent classification — is the task of taking a written or spoken input, and classifying it based on what the user wants to achieve. Intent recognition forms an essential component of chatbots and finds use in sales conversions, customer support, and many other areas"
      ],
      "metadata": {
        "id": "ETe8VBjCtDAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*12. Discuss the advantages of using word embeddings in text preprocessing.*"
      ],
      "metadata": {
        "id": "Xbt2MOr6tGq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What Are Word Embeddings and why Are They Useful? | by Diogo ...\n",
        "In Agent Assist, Word Embeddings help us understand the meaning of each word, which can be used to recommend articles, suggest automations, and enable more features based on the dialogue meaning."
      ],
      "metadata": {
        "id": "g8RXGfYytKJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. How do RNN-based techniques handle sequential information in text processing tasks?"
      ],
      "metadata": {
        "id": "0RcRbilxtP5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent neural networks (RNNs) are the state of the art algorithm for sequential data and are used by Apple's Siri and Google's voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.\n",
        "\n",
        "RNN maintains internal memory, due to this they are very efficient for machine learning problems that involve sequential data. RNNs are also used in time series predictions as well. The main advantage of using RNNs instead of standard neural networks is that the features are not shared in standard neural networks."
      ],
      "metadata": {
        "id": "ZIJqlNhntUNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*14. What is the role of the encoder in the encoder-decoder architecture?*"
      ],
      "metadata": {
        "id": "L9mGaDYotfdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-decoder architectures can handle inputs and outputs that both consist of variable-length sequences and thus are suitable for seq2seq problems such as machine translation. The encoder takes a variable-length sequence as input and transforms it into a state with a fixed shape.\n",
        "\n",
        "In this architecture, the encoder maps the input to a fixed-length representation, which is then passed to the decoder to generate the output"
      ],
      "metadata": {
        "id": "5vYpupI9tsTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*15. Explain the concept of attention-based mechanism and its significance in text processing.*"
      ],
      "metadata": {
        "id": "GfJ6iCTJt06v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention mechanism allows the model to \"pay attention\" to certain parts of the data and to give them more weight when making predictions. In a nutshell, the attention mechanism helps preserve the context of every word in a sentence by assigning an attention weight relative to all other words.\n",
        "\n",
        "Attention mechanisms enhance deep learning models by selectively focusing on important input elements, improving prediction accuracy and computational efficiency. They prioritize and emphasize relevant information, acting as a spotlight to enhance overall model performance."
      ],
      "metadata": {
        "id": "M9YFbI-Yt8Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*16. How does self-attention mechanism capture dependencies between words in a text?*"
      ],
      "metadata": {
        "id": "cooEa654uIHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism uses three matrices - query (Q), key (K), and value (V) - to help the system understand and process the relationships between words in a sentence. These three matrices serve distinct purposes: Query (Q): This matrix represents the focus word for which the context is being determined.\n",
        "\n",
        "\n",
        "In other words, self-attention layer differentiably key-value searches the input sequence for each inputs, and adds results to the output sequence. Output and input have the same sequence length and dimension. Weight each value by similarity of the corresponding query and key."
      ],
      "metadata": {
        "id": "OqZ-CnseuLyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Discuss the advantages of the transformer architecture over traditional RNN-based models."
      ],
      "metadata": {
        "id": "Uy8_Y69vuYWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages: better representation of input tokens where the token representation is based on specific neighboring tokens using self-attention. attends (in parallel) to all input tokens, as opposed to being limited by memory issues from sequential processing (RNNs)..\n",
        "\n",
        "Like recurrent neural networks (RNNs), Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, Transformers do not require that the sequential data be processed in order.\n"
      ],
      "metadata": {
        "id": "Bo3tqfmnufVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*18. What are some applications of text generation using generative-based approaches?*"
      ],
      "metadata": {
        "id": "IxOjj1HpusvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Potential applications: Data augmentation, dataset synthesis, art creation, code generation, text generation, audio synthesis, etc. Synthesized by score-based generative models.\n",
        "\n",
        "Blog posts based on keywords and the desired length.\n",
        "Product descriptions based on data about its features and benefits.\n",
        "Social media posts.\n",
        "Media campaigns (e.g. ads) ...\n",
        "Reports like regional sales reports. ...\n",
        "Automated article generation for regular events like sports matches."
      ],
      "metadata": {
        "id": "WyHXuKjJwv4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*19. How can generative models be applied in conversation AI systems?*"
      ],
      "metadata": {
        "id": "5GuNY1403gr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversational agents: Generative AI models can be used to develop virtual assistants and chatbots that can automatically respond to user inquiries and hold natural conversations. Translation: Generative AI models can swiftly and accurately translate text from one language to another."
      ],
      "metadata": {
        "id": "qsgIvZn-3lCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*20. Explain the concept of natural language understanding (NLU) in the context of conversation AI.*"
      ],
      "metadata": {
        "id": "9E_IEfKL3rTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLU enables human-computer interaction. It is the comprehension of human language such as English, Spanish and French, for example, that allows computers to understand commands without the formalized syntax of computer languages. NLU also enables computers to communicate back to humans in their own languages.\n",
        "\n",
        "NLU is branch of natural language processing (NLP), which helps computers understand and interpret human language by breaking down the elemental pieces of speech. While speech recognition captures spoken language in real-time, transcribes it, and returns text, NLU goes beyond recognition to determine a user's intent."
      ],
      "metadata": {
        "id": "gpioI7Pe3uc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*21. What are some challenges in building conversation AI systems for different languages or domains?*"
      ],
      "metadata": {
        "id": "rbubteiH36dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other problems of conversational AI platforms:\n",
        "\n",
        "Regional jargon and slang.\n",
        "\n",
        "Dialects not conforming to standard language.\n",
        "\n",
        "Background noise distorting the voice of the speaker.\n",
        "\n",
        "Unscripted questions that the virtual assistant or chatbot does not know to answer.\n",
        "\n",
        "Unplanned responses by customers."
      ],
      "metadata": {
        "id": "_gayQ07E39sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*22. Discuss the role of word embeddings in sentiment analysis tasks.*"
      ],
      "metadata": {
        "id": "nwfw0pJP4HVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Word embeddings can be used in sentiment analysis to represent the words in a piece of text in a continuous vector space, capturing the relationships between words and their meanings. In sentiment analysis, the goal is to classify text as having positive, negative, or neutral sentiment.\n",
        "\n",
        "\n",
        "A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level—whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral."
      ],
      "metadata": {
        "id": "Rzbof7hK4OA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*23. How do RNN-based techniques handle long-term dependencies in text processing?*"
      ],
      "metadata": {
        "id": "A_HdvIQ24aZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Since Recurrent Neural Network (RNN) suffers from vanishing gradient problems blocking the network to learn long-timescale dependencies while, LSTM reduces this problem by introducing forget gate, input gate, and output gate.\n",
        "\n",
        "In theory, RNN's are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, recurrent neural network don't seem to be able to learn them. This problem is called Vanishing gradient problem"
      ],
      "metadata": {
        "id": "0U1ag7pe4dzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*24. Explain the concept of sequence-to-sequence models in text processing tasks.*"
      ],
      "metadata": {
        "id": "m-6yLF7L5B7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc.\n",
        "\n",
        "\n",
        "The seq2seq (sequence to sequence) model is a type of encoder-decoder deep learning model commonly employed in natural language processing that uses recurrent neural networks like LSTM to generate output. seq2seq can generate output token by token or character by character."
      ],
      "metadata": {
        "id": "eQpC7C1r5LhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*25. What is the significance of attention-based mechanisms in machine translation tasks?*"
      ],
      "metadata": {
        "id": "cZLB-xgX5URg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine translation, attention mechanism is used to align and selectively focus on relevant parts of the source sentence during the translation process. It allows the model to assign weights to more important words or phrases.\n",
        "\n",
        "\n",
        "A Guide to Neural Machine Translation With Attention ...\n",
        "Based on this vector, the decoder produces words one by one to create the output sentence. Throughout this process, the attention mechanism helps the decoder focus on different fragments of the input sentence."
      ],
      "metadata": {
        "id": "X--lWzny5Z-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*26. Discuss the challenges and techniques involved in training generative-based models for text generation*"
      ],
      "metadata": {
        "id": "tVDGszNr5jEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text generation works by utilizing algorithms and language models to process input data and generate output text. It involves training AI models on large datasets of text to learn patterns, grammar, and contextual information.\n",
        "\n",
        "\n",
        "There are four major approaches to textual analysis: rhetorical criticism, content analysis, interaction analysis, and performance studies"
      ],
      "metadata": {
        "id": "lY0eIZRe5mxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. How can conversation AI systems be evaluated for their performance and effectiveness?\n"
      ],
      "metadata": {
        "id": "BU0YV0ft5zxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversational Recommender Systems (CRS) support people to interact with information through conversations to achieve recommendation-related goals. These systems are able to capture rich contextual information and model users' preferences through natural language interactions\n",
        "\n",
        "\n",
        "Conversational Systems are intelligent machines that can understand language and conduct a written or verbal conversation with a customer. Their use is aimed at improving customer experience by steering interaction."
      ],
      "metadata": {
        "id": "8x1XwAdJ5-JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*28. Explain the concept of transfer learning in the context of text preprocessing.*"
      ],
      "metadata": {
        "id": "LskaPl6H6MV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What Is Transfer Learning? A Guide for Deep Learning | Built In\n",
        "Transfer learning, used in machine learning, is the reuse of a pre-trained model on a new problem. In transfer learning, a machine exploits the knowledge gained from a previous task to improve generalization about another.\n",
        "\n",
        "\n",
        "Now consider what might happen if the algorithms had a way to apply the knowledge they learned from solving one task to another that was similar but distinct. In NLP, pre-trained language models aid us in doing this, and in the realm of deep learning, this concept is referred to as transfer learning"
      ],
      "metadata": {
        "id": "lXCj-srx6QQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*29. What are some challenges in implementing attention-based mechanisms in text processing models?*"
      ],
      "metadata": {
        "id": "STRK0aeq6Z97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "However, attention mechanisms also have some challenges and limitations to consider. For instance, they can increase the computational cost and complexity of seq2seq models, by adding more parameters and operations, as well as requiring more memory and time.\n",
        "\n",
        "\n",
        "ATTENTION FUNCTION. The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements."
      ],
      "metadata": {
        "id": "ij1E0Yv_6esJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Discuss the role of conversation AI in enhancing user experiences and interactions on social media platforms."
      ],
      "metadata": {
        "id": "RSpV7xmV6pU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI tools help enhance features of social media platforms and lead social media activities at scale across a number of use cases, including text and visual content creation, social media monitoring, ad management, influencer research, brand awareness campaigns and more\n",
        "\n",
        "\n",
        "Conversational AI technology can help mitigate the risk of human error and improve customer service. By automating routine tasks, such as data entry and transaction processing, banks can reduce the likelihood of errors caused by human fatigue or oversight."
      ],
      "metadata": {
        "id": "q9TsZmXV6skz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4S6LQ7T764Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IfshC6yH6pgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uEEPdO1Jt1E9"
      }
    }
  ]
}