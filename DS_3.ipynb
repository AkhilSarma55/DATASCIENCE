{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*1.What is the purpose of the General Linear Model (GLM)*"
      ],
      "metadata": {
        "id": "mgBzpsznWkJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generalized linear models (GLMs) allow the extension of linear modeling ideas to a wider class of response types, such as count data or binary responses\n",
        "\n",
        "The general linear model and the generalized linear model (GLM) are two commonly used families of statistical methods to relate some number of continuous and/or categorical predictors to a single outcome variable."
      ],
      "metadata": {
        "id": "-WaeHlIqWpy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2.What are the key assumptions of the General Linear Model*"
      ],
      "metadata": {
        "id": "d1zeqkMDW3RK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The general linear model fitted using ordinary least squares (which includes Student's t test, ANOVA, and linear regression) makes four assumptions: linearity, homoskedasticity (constant variance), normality, and independence."
      ],
      "metadata": {
        "id": "xLK7kAhTW493"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3.How do you interpret the coefficients in a GLM***"
      ],
      "metadata": {
        "id": "rkz-RpOFXOto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case you can interpret the coefficients as multiplying the probabilities by exp(β1) e x p ( β 1 ) , however these models can give you predicted probabilities greater than 1, and often don't converge."
      ],
      "metadata": {
        "id": "pZTWaqQBXiFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***4.what is the difference between a univariate and multivariate glm***"
      ],
      "metadata": {
        "id": "FuFpQSdKYkkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The term univariate analysis refers to the analysis of one variable. You can remember this because the prefix “uni” means “one.” The term multivariate analysis refers to the analysis of more than one variable. You can remember this because the prefix “multi” means “more than one.\n",
        "\n",
        "Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables."
      ],
      "metadata": {
        "id": "ZlW-uMu5Ywpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***5.Explain the concept of interaction effects in a GLM.***"
      ],
      "metadata": {
        "id": "JuhiXQlqm7jY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, the existence of an interaction means that the effect of one variable depends on the value of the other variable with which it interacts. If there isn't an interaction, then the value of the other variable doesn't matter. This is easiest to understand in the case of linear regression"
      ],
      "metadata": {
        "id": "CrTo4mivnBAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***6.How do you handle categorical predictors in a GLM***"
      ],
      "metadata": {
        "id": "yxKDrlffnKgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Understand your data. ...\n",
        "\n",
        "2 Choose the appropriate encoding technique. ...\n",
        "\n",
        "3 Handle missing values. ...\n",
        "\n",
        "4 Avoid overfitting and leakage. ...\n",
        "\n",
        "5 Feature scaling."
      ],
      "metadata": {
        "id": "4twDdOErnRBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***7.What is the purpose of the design matrix in a GLM***"
      ],
      "metadata": {
        "id": "R_kxN4p4ndx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The design matrix is used in certain statistical models, e.g., the general linear model. It can contain indicator variables (ones and zeros) that indicate group membership in an ANOVA, or it can contain values of continuous variables.\n",
        "\n",
        "The purpose of the design matrix is to allow models that further constrain parameter sets. These constraints provide additional flexibility in modeling and allows researchers to build models that cannot be derived using the simple PIMs in"
      ],
      "metadata": {
        "id": "ldx3Qym4njUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***8.How do you test the significance of predictors in a GLM***"
      ],
      "metadata": {
        "id": "msMfLJDnn0HF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A low p-value (< 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable"
      ],
      "metadata": {
        "id": "3rVenVcrn4hW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM***"
      ],
      "metadata": {
        "id": "EuVe7NX9oC3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical (Type II) involves comparing the change in SS to a model with all other effects of equal or lower order (e.g., three-way interactions, two-way interactions and main effects). Unique SS (Type III) compares SS with a model containing all other effects (regardless of order).\n",
        "\n",
        "Type I sum of squares are “sequential.” In essence the factors are tested in the order they are listed in the model. Type III sum of squares are “partial.” In essence, every term in the model is tested in light of every other term in the model."
      ],
      "metadata": {
        "id": "oE9ugufOoJeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***10.Explain the concept of deviance in a GLM.***"
      ],
      "metadata": {
        "id": "T_QAV4fxoj2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deviance is a goodness-of-fit metric for statistical models, particularly used for GLMs. It is defined as the difference between the Saturated and Proposed Models and can be thought as how much variation in the data does our Proposed Model account for. Therefore, the lower the deviance, the better the model.\n",
        "\n",
        "\n",
        "The fitted values produced by the model are most unlikely to match the values of the data perfectly. The size of the discrepancy between the model and the data is a measure of the inadequacy of the model; a small discrepancy may be tolerable, but a large one will not be."
      ],
      "metadata": {
        "id": "13__KvO_oq3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***11.What is regression analysis and what is its purpose***"
      ],
      "metadata": {
        "id": "DX4HaXI0o0Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression analysis is a statistical method that shows the relationship between two or more variables. Usually expressed in a graph, the method tests the relationship between a dependent variable against independent variables."
      ],
      "metadata": {
        "id": "OMoPuZKpo5u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***12.. What is the difference between simple linear regression and multiple linear regression***"
      ],
      "metadata": {
        "id": "9f1SRncgo-_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple linear regression can occur in an infinite number of dimensions. They both have one Y variable that is explained by either 1, or many X variables. The similarities between simple linear and multiple regression are: Both models predict a dependent variable with the help of independent variable..\n",
        "\n",
        "Simple regression has one predictor variable and one variable you are trying to predict. Multiple regression has more than that. It is also called simple linear regression. It establishes the relationship between two variables using a straight line."
      ],
      "metadata": {
        "id": "hpvjgKKhpD_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***13.How do you interpret the R-squared value in regression***"
      ],
      "metadata": {
        "id": "Il14pxEnqNHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common interpretation of r-squared is how well the regression model explains observed data. For example, an r-squared of 60% reveals that 60% of the variability observed in the target variable is explained by the regression model.\n",
        "\n",
        " The coefficient of determination, or R2 , is a measure that provides information about the goodness of fit of a model. In the context of regression it is a statistical measure of how well the regression line approximates the actual data."
      ],
      "metadata": {
        "id": "CJfHviauqPRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***14.What is the difference between correlation and regression***"
      ],
      "metadata": {
        "id": "i_Jg3CVyqZf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference between correlation and regression is that correlation measures the degree of a relationship between two independent variables (x and y). In contrast, regression is how one variable affects another"
      ],
      "metadata": {
        "id": "1g8IsW4Fqd9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***15.What is the difference between the coefficients and the intercept in regression***"
      ],
      "metadata": {
        "id": "eQNmBUntqmCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simple linear regression model is essentially a linear equation of the form y = c + b*x; where y is the dependent variable (outcome), x is the independent variable (predictor), b is the slope of the line; also known as regression coefficient and c is the intercept; labeled as constant."
      ],
      "metadata": {
        "id": "HQYUCj2ZqolS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***16.How do you handle outliers in regression analysis***"
      ],
      "metadata": {
        "id": "2z0KQHCJqv-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many possible approaches to dealing with outliers: removing them from the observations, treating them (for example, capping the extreme observations at a reasonable value), or using algorithms that are well-suited for dealing with such values on their own."
      ],
      "metadata": {
        "id": "pntRe_Maqytu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***17.What is the difference between ridge regression and ordinary least squares regression***"
      ],
      "metadata": {
        "id": "DTuHtT0iq2-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "when there is a difference in variance between predictor variables, OLS tends to give higher variance for coefficients corresponding to predictors with higher variance, while Ridge Regression reduces the variance differences between coefficients by shrinking them towards zero"
      ],
      "metadata": {
        "id": "JTJNM1x2q8BL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***18.What is heteroscedasticity in regression and how does it affect the model***"
      ],
      "metadata": {
        "id": "Osik4sAhrINU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroskedasticity refers to a situation where the variance of the residuals is unequal over a range of measured values. If heteroskedasticity exists, the population used in the regression contains unequal variance, the analysis results may be invalid.\n",
        "\n",
        "\n",
        "Heteroscedasticity makes a regression model less dependable because the residuals should not follow any specific pattern. The scattering should be random around the fitted line for the model to be robust. One very popular way to deal with heteroscedasticity is to transform the dependent variable [2]."
      ],
      "metadata": {
        "id": "AEMlki7OrNTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***19.How do you handle multicollinearity in regression analysis***"
      ],
      "metadata": {
        "id": "lM1x91eVrXNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix multicollinearity, one can remove one of the highly correlated variables, combine them into a single variable, or use a dimensionality reduction technique such as principal component analysis to reduce the number of variables while retaining most of the information"
      ],
      "metadata": {
        "id": "rMvhk9K8rdFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***20.What is polynomial regression and when is it used***"
      ],
      "metadata": {
        "id": "5_HuoBL_rhM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship."
      ],
      "metadata": {
        "id": "5RvxftccrnkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***21.What is a loss function and what is its purpose in machine learning***"
      ],
      "metadata": {
        "id": "7C1YF5Serroc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value). We convert the learning problem into an optimization problem, define a loss function and then optimize the algorithm to minimize the loss function."
      ],
      "metadata": {
        "id": "RnkjdcrNr3s4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***22.What is the difference between a convex and non-convex loss function***"
      ],
      "metadata": {
        "id": "EemjYC5er672"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A convex function: given any two points on the curve there will be no intersection with any other points, for non convex function there will be at least one intersection. In terms of cost function with a convex type you are always guaranteed to have a global minimum, whilst for a non convex only local minima."
      ],
      "metadata": {
        "id": "uRtuTjf7sB5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***23.What is mean squared error (MSE) and how is it calculated***"
      ],
      "metadata": {
        "id": "5hbefJDuorEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Mean Squared Error measures how close a regression line is to a set of data points. It is a risk function corresponding to the expected value of the squared error loss. Mean square error is calculated by taking the average, specifically the mean, of errors squared from data as it relates to a function.\n",
        "\n",
        "\n",
        "MSE is the average of the squared error that is used as the loss function for least squares regression: It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE."
      ],
      "metadata": {
        "id": "n1aW6tHbsRHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***24. What is mean absolute error (MAE) and how is it calculated***"
      ],
      "metadata": {
        "id": "ptyAIKBqswJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Absolute Error (MAE) is calculated by taking the summation of the absolute difference between the actual and calculated values of each observation over the entire array and then dividing the sum obtained by the number of observations in the array."
      ],
      "metadata": {
        "id": "ogARgIE8s1x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***25.What is log loss (cross-entropy loss) and how is it calculated***"
      ],
      "metadata": {
        "id": "YmnJMRtFs8IZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log Loss (Binary Cross-Entropy Loss): A loss function that represents how much the predicted probabilities deviate from the true ones. It is used in binary cases. Cross-Entropy Loss: A generalized form of the log loss, which is used for multi-class classification problems."
      ],
      "metadata": {
        "id": "pR8YiizYtBLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***26. How do you choose the appropriate loss function for a given problem***"
      ],
      "metadata": {
        "id": "W8uqJHqltH-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's no one-size-fits-all loss function to algorithms in machine learning. There are various factors involved in choosing a loss function for specific problem such as type of machine learning algorithm chosen, ease of calculating the derivatives and to some degree the percentage of outliers in the data set"
      ],
      "metadata": {
        "id": "2AkHLYZJtPQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***27.Explain the concept of regularization in the context of loss functions***"
      ],
      "metadata": {
        "id": "55DfspSPtURl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting.\n",
        "\n",
        "The loss function is a penalty between a model and the truth. This can be a prediction of class conditional distribution vs true label, thus can also be a data distribution vs. empirical sample, and many more. Regularization is a term, penalty, measure which is supposed to be a penalty for the too complex model"
      ],
      "metadata": {
        "id": "29Ib7dX7m7mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***28.What is Huber loss and how does it handle outliers***"
      ],
      "metadata": {
        "id": "iz96WQS4txhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huber loss, named after its creator Peter J. Huber, is a robust loss function that combines the best properties of L1 and L2 losses. It is less sensitive to outliers in data than the mean squared error loss, making it ideal for regression problems where data can have large outliers."
      ],
      "metadata": {
        "id": "MwCtBcCjt5Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***29.What is quantile loss and when is it used***"
      ],
      "metadata": {
        "id": "2IBCZHFKuBxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most popular quantile is the median, or the 50th percentile, and in this case the quantile loss is simply the sum of absolute errors. Other quantiles could give endpoints of a prediction interval; for example a middle-80-percent range is defined by the 10th and 90th percentiles.\n",
        "\n",
        "As the name suggests, the quantile regression loss function is applied to predict quantiles. A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the time"
      ],
      "metadata": {
        "id": "ugrXVo8guG-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***30.What is the difference between squared loss and absolute loss***"
      ],
      "metadata": {
        "id": "yIu9T7eWubGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the prediction error causes the client's loss (e.g. financial loss) to grow quadratically and symmetrically about zero, you are facing square prediction loss. If the client's loss grows linearly and symmetrically about zero, you are facing absolute prediction loss."
      ],
      "metadata": {
        "id": "kXaazbv4ugZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***31. What is an optimizer and what is its purpose in machine learning***"
      ],
      "metadata": {
        "id": "Q1rH48_TunNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss. But it is a daunting task to choose the appropriate weights for the model."
      ],
      "metadata": {
        "id": "hZrzHfswu7ZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**32.What is Gradient Descent (GD) and how does it work**"
      ],
      "metadata": {
        "id": "i1CH2gFXvBnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates."
      ],
      "metadata": {
        "id": "xQ2ep1_vvJ_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***33. What are the different variations of Gradient Descen***"
      ],
      "metadata": {
        "id": "eP9nhV1hvmRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three types of gradient descent learning algorithms: batch gradient descent, stochastic gradient descent and mini-batch gradient descent."
      ],
      "metadata": {
        "id": "J2xanRM8vsfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***34.What is the learning rate in GD and how do you choose an appropriate value***"
      ],
      "metadata": {
        "id": "dPrcRqP4v0aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose a Fixed Learning Rate. The standard gradient descent procedure uses a fixed learning rate (e.g. 0.01) that is determined by trial and error. ...\n",
        "\n",
        "Use Learning Rate Annealing. ...\n",
        "\n",
        "Use Cyclical Learning Rates. ...\n",
        "\n",
        "Use an Adaptive Learning Rate. ...\n",
        "\n",
        "References."
      ],
      "metadata": {
        "id": "4JDH2H5Pv8PT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2u0QIB7JwG36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HVP6JuKgwG7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***35.How does GD handle local optima in optimization problems***"
      ],
      "metadata": {
        "id": "jijrjZndwHHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum, simply put, adds a fraction of the past weight update to the current weight update. This helps prevent the model from getting stuck in local minima, as even if the current gradient is 0, the past one most likely was not, so it will as easily get stuck."
      ],
      "metadata": {
        "id": "jDdxNxfswOhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***36.What is Stochastic Gradient Descent (SGD) and how does it differ from GD***"
      ],
      "metadata": {
        "id": "roqqLafXwioZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent is a drastic simplification of GD which overcomes some of its difficulties. Each iteration of SGD computes the gradient on the basis of one randomly chosen partition of the dataset which was shuffled, instead of using the whole part of the observations."
      ],
      "metadata": {
        "id": "McxTRrJgwn13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***38.What is the role of momentum in optimization algorithms***"
      ],
      "metadata": {
        "id": "V0H-pqP7wsl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum is a strategy for accelerating the convergence of the optimization process by including a momentum element in the update rule. This momentum factor assists the optimizer in continuing to go in the same direction even if the gradient changes direction or becomes zero."
      ],
      "metadata": {
        "id": "3-3wBV94w5vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***39.What is the difference between batch GD, mini-batch GD, and SGD***"
      ],
      "metadata": {
        "id": "-bi6KM3aw_zR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With little error, GD modifies the model's parameters based on the average of all training samples. Due to the fact that SGD is updated using just one training sample, it has a lot of noise. Mini-batch Gradient Descent has a significant amount of noise because the update is based on a small number of training example"
      ],
      "metadata": {
        "id": "MqNRTes4xEoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***40. How does the learning rate affect the convergence of GD***"
      ],
      "metadata": {
        "id": "at53NHgjxLMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With gradient descent, if the learning rate is too small, the weights will be updated very slowly hence convergence takes a lot of time even when the gradient is high. This is shown in the left side image below. If the learning rate is too high cost oscillates around the minima as shown in the right side image below"
      ],
      "metadata": {
        "id": "wIIpzqsOxTIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***41.What is regularization and why is it used in machine learning***"
      ],
      "metadata": {
        "id": "u0RAwy4HxXn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it."
      ],
      "metadata": {
        "id": "259c-oRoxgKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***42.What is the difference between L1 and L2 regularization***"
      ],
      "metadata": {
        "id": "JSlneP2VxkL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights."
      ],
      "metadata": {
        "id": "r_Q-SBY1xp9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D6rwVRcXxtye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***43. Explain the concept of ridge regression and its role in regularization***"
      ],
      "metadata": {
        "id": "OHbmjJtDxt8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values."
      ],
      "metadata": {
        "id": "PqIbM6Wlx0dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***44.What is the elastic net regularization and how does it combine L1 and L2 penalties***"
      ],
      "metadata": {
        "id": "3q2rXKawx7so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods."
      ],
      "metadata": {
        "id": "t4KrWspl8EaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***45.How does regularization help prevent overfitting in machine learning models***"
      ],
      "metadata": {
        "id": "jnaxmk_g8Hfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2 regularization, also known as Ridge Regression, adds a penalty term proportional to the square of the model's parameters. This encourages the model to use all of the parameters but to reduce their values, resulting in a model that is less complex and less prone to overfitting."
      ],
      "metadata": {
        "id": "YZUfOyEFB_sK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***46.What is early stopping and how does it relate to regularization***"
      ],
      "metadata": {
        "id": "eM0D08xmCEmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration."
      ],
      "metadata": {
        "id": "7_zUOfcpCKME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***47.Explain the concept of dropout regularization in neural networks.***"
      ],
      "metadata": {
        "id": "lobfz4GRCOcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, “dropout” refers to the practice of disregarding certain nodes in a layer at random during training. A dropout is a regularization approach that prevents overfitting by ensuring that no units are codependent with one another"
      ],
      "metadata": {
        "id": "QRMma9mMCTCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How do you choose the regularization parameter in a model***"
      ],
      "metadata": {
        "id": "QyUoEUhwCW-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "on the training set, we estimate several different Ridge regressions, with different values of the regularization parameter; on the validation set, we choose the best model (the regularization parameter which gives the lowest MSE on the validation set);"
      ],
      "metadata": {
        "id": "7w4lkNM7CdNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is the trade-off between bias and variance in regularized models***"
      ],
      "metadata": {
        "id": "pfoweAw1CggR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias"
      ],
      "metadata": {
        "id": "dMuygMk0Cmcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is Support Vector Machines (SVM) and how does it work***"
      ],
      "metadata": {
        "id": "rEv6-VoiCsfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM is a supervised learning algorithm, that can be used for both classification as well as regression problems. However, mostly it is used for classification problems. It is a highly efficient and preferred algorithm due to significant accuracy with less computation power."
      ],
      "metadata": {
        "id": "fzAkK-ODC3M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How does the kernel trick work in SVM***"
      ],
      "metadata": {
        "id": "IGL1YMsFC68O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The “trick” is that kernel methods represent the data only through a set of pairwise similarity comparisons between the original data observations x (with the original coordinates in the lower dimensional space), instead of explicitly applying the transformations ϕ(x) and representing the data by these transformed ..."
      ],
      "metadata": {
        "id": "90X7JDWmC_iR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What are support vectors in SVM and why are they important***"
      ],
      "metadata": {
        "id": "W3sEN1ZuDDA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier."
      ],
      "metadata": {
        "id": "B9lOqSmRDHOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explain the concept of the margin in SVM and its impact on model performance.***"
      ],
      "metadata": {
        "id": "pmredlNjDMOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Margin: it is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin. There are two types of margins hard margin and soft margin"
      ],
      "metadata": {
        "id": "mgjZrT-IDPcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How do you handle unbalanced datasets in SVM***"
      ],
      "metadata": {
        "id": "devAa81KDUBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the optimal separating hyperplane using an SVC for classes that are unbalanced. We first find the separating plane with a plain SVC and then plot (dashed) the separating hyperplane with automatically correction for unbalanced classes."
      ],
      "metadata": {
        "id": "KQ4d98k6DXhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is the difference between linear SVM and non-linear SVM***"
      ],
      "metadata": {
        "id": "sd0JSIB0DdOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear SVM: When the data points are linearly separable into two classes, the data is called linearly-separable data. We use the linear SVM classifier to classify such data. Non-linear SVM: When the data is not linearly separable, we use the non-linear SVM classifier to separate the data points."
      ],
      "metadata": {
        "id": "SjfnexuPDmZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is the role of C-parameter in SVM and how does it affect the decision boundary***"
      ],
      "metadata": {
        "id": "K7Vg7TXMDqA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C parameter adds a penalty for each misclassified data point. If c is small, the penalty for misclassified points is low so a decision boundary with a large margin is chosen at the expense of a greater number of misclassifications ."
      ],
      "metadata": {
        "id": "0ZYEqXPJDvJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explain the concept of slack variables in SVM***"
      ],
      "metadata": {
        "id": "WJhnQHCiDyOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slack variables are introduced to allow certain constraints to be violated. That is, certain train- ing points will be allowed to be within the margin. We want the number of points within the margin to be as small as possible, and of course we want their penetration of the margin to be as small as possible."
      ],
      "metadata": {
        "id": "i84_Bmm9D2Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is the difference between hard margin and soft margin in SVM***"
      ],
      "metadata": {
        "id": "VXAzhKsyD6HJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hard Margin Classification only works if the data is linearly separable also Hard Margins are very sensitive to outliers. We can use soft margin classifications to avoid these issues. To avoid issues it is recommended to use a more flexible model with soft margin classifications."
      ],
      "metadata": {
        "id": "jtSq4C2ED_dg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How do you interpret the coefficients in an SVM model***"
      ],
      "metadata": {
        "id": "Yi9gk3rAEFBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that in linear SVM, the result is a hyperplane that separates the classes as best as possible. The weights represent this hyperplane, by giving you the coordinates of a vector which is orthogonal to the hyperplane - these are the coefficients given by svm."
      ],
      "metadata": {
        "id": "y7eRTZoOELsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is a decision tree and how does it work***"
      ],
      "metadata": {
        "id": "vUk3D9aSEOhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree is a type of supervised machine learning used to categorize or make predictions based on how a previous set of questions were answered. The model is a form of supervised learning, meaning that the model is trained and tested on a set of data that contains the desired categorization."
      ],
      "metadata": {
        "id": "sydvFHEzETpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees**"
      ],
      "metadata": {
        "id": "f5KJdZq8EWS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini index and entropy is the criterion for calculating information gain. Decision tree algorithms use information gain to split a node. Both gini and entropy are measures of impurity of a node. A node having multiple classes is impure whereas a node having only one class is pure."
      ],
      "metadata": {
        "id": "zs1e4VJ7Eeas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explain the concept of information gain in decision trees.***"
      ],
      "metadata": {
        "id": "NsosorWxEhqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nformation gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node."
      ],
      "metadata": {
        "id": "AsXYDlwtEmfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How do you handle missing values in decision trees***"
      ],
      "metadata": {
        "id": "XJ7zc9ULEqcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to handle missing values is to add a preprocessing step to treat for them in our machine learning pipeline. In fact, many machine learning algorithms will require such a step to be added in order to prevent software failures during operations."
      ],
      "metadata": {
        "id": "ICY3PigGEvQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is pruning in decision trees and why is it important***"
      ],
      "metadata": {
        "id": "q2GiANl6EyZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning and data mining, pruning is a technique associated with decision trees. Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances."
      ],
      "metadata": {
        "id": "W7LOwhMRE342"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is the difference between a classification tree and a regression tree***"
      ],
      "metadata": {
        "id": "JLKcP50EE7C7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification trees and regression trees are two types of decision trees that can be used to construct a decision graph. A classification tree is used when the output variable is categorical, while a regression tree is used when the output variable is continuous."
      ],
      "metadata": {
        "id": "p_WpgcuHFBQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How do you interpret the decision boundaries in a decision tree***"
      ],
      "metadata": {
        "id": "OUIQxHPAFGEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision boundary of a decision tree is determined by overlapping orthogonal half-planes (representing the result of each subsequent decision) and can end up as displayed on the pictures"
      ],
      "metadata": {
        "id": "6-k7B1gMFKhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is the role of feature importance in decision trees***"
      ],
      "metadata": {
        "id": "Rtuz18A_FO6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A decision tree is explainable machine learning algorithm all by itself. Beyond its transparency, feature importance is a common way to explain built models as well. Coefficients of linear regression equation give a opinion about feature importance but that would fail for non-linear models."
      ],
      "metadata": {
        "id": "LzQ0kSXNFU_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What are ensemble techniques and how are they related to decision trees***"
      ],
      "metadata": {
        "id": "4wsuTpmWFZ4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques address the limitations of individual decision trees by combining multiple models to make more accurate predictions. The fundamental idea is that the collective wisdom of several models is often superior to that of a single model."
      ],
      "metadata": {
        "id": "0zL5xWcAFfUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are ensemble techniques in machine learninga**"
      ],
      "metadata": {
        "id": "7LWTbZJbFlDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this ensemble technique, machine learning professionals use a number of models for making predictions about each data point. The predictions made by different models are taken as separate votes. Subsequently, the prediction made by most models is treated as the ultimate prediction"
      ],
      "metadata": {
        "id": "k8zbAHfJFsQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is bagging and how is it used in ensemble learning***"
      ],
      "metadata": {
        "id": "1Pc1hdLdFwtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy dataset. In bagging, a random sample of data in a training set is selected with replacement—meaning that the individual data points can be chosen more than once."
      ],
      "metadata": {
        "id": "n5kpr-GJF1q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Explain the concept of bootstrapping in bagging***"
      ],
      "metadata": {
        "id": "tSflt8q-F49l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging is composed of two parts: aggregation and bootstrapping. Bootstrapping is a sampling method, where a sample is chosen out of a set, using the replacement method. The learning algorithm is then run on the samples selected."
      ],
      "metadata": {
        "id": "wZKAueqwF-jB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is boosting and how does it work"
      ],
      "metadata": {
        "id": "aqTL8SuLGMQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting creates an ensemble model by combining several weak decision trees sequentially. It assigns weights to the output of individual trees. Then it gives incorrect classifications from the first decision tree a higher weight and input to the next tree."
      ],
      "metadata": {
        "id": "dvoQEXkMGTJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is the difference between AdaBoost and Gradient Boosting***"
      ],
      "metadata": {
        "id": "GJvravBhGX0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost is the first designed boosting algorithm with a particular loss function. On the other hand, Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem. This makes Gradient Boosting more flexible than AdaBoost."
      ],
      "metadata": {
        "id": "fdWTWCcQGdfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** What is the purpose of random forests in ensemble learning***"
      ],
      "metadata": {
        "id": "ZDw_c24NGiNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest algorithm is an ensemble learning technique combining numerous classifiers to enhance a model's performance. Random Forest is a supervised machine-learning algorithm made up of decision trees. Random Forest is used for both classification and regression problems."
      ],
      "metadata": {
        "id": "YpdoPTsTGtZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How do random forests handle feature importance***"
      ],
      "metadata": {
        "id": "os2pR7G7Gw5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable."
      ],
      "metadata": {
        "id": "q7hllWpTG2wH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is stacking in ensemble learning and how does it work***"
      ],
      "metadata": {
        "id": "EExVbxkXG6KB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking is one of the most popular ensemble machine learning techniques used to predict multiple nodes to build a new model and improve model performance. Stacking enables us to train multiple models to solve similar problems, and based on their combined output, it builds a new model with improved performance."
      ],
      "metadata": {
        "id": "GCQriVSaG_N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What are the advantages and disadvantages of ensemble techniques***"
      ],
      "metadata": {
        "id": "8pxpMuEsHB6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble methods offer several advantages over single models, such as improved accuracy and performance, especially for complex and noisy problems. They can also reduce the risk of overfitting and underfitting by balancing the trade-off between bias and variance, and by using different subsets and features of the data."
      ],
      "metadata": {
        "id": "pXdjbEwVHF3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How do you choose the optimal number of models in an ensemble***"
      ],
      "metadata": {
        "id": "X-xxWtouHSEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 : Find the KS of individual models. ...\n",
        "\n",
        "Step 2: Index all the models for easy access. ...\n",
        "\n",
        "Step 3: Choose the first two models as the initial selection and set a correlation limit. ...\n",
        "\n",
        "Step 4: Iteratively choose all the models which are not highly correlated with any of the any chosen model."
      ],
      "metadata": {
        "id": "BYP-hFM2HXI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7zYG7CPQHgO7"
      }
    }
  ]
}